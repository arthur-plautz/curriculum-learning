{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specialist_data import manager\n",
    "from models.specialist import Specialist\n",
    "\n",
    "batch_size = 50\n",
    "interval = batch_size/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67317385\n",
      "Iteration 2, loss = 0.71860285\n",
      "Iteration 3, loss = 0.71027317\n",
      "Iteration 4, loss = 0.71041083\n",
      "Iteration 5, loss = 0.70872635\n",
      "Iteration 6, loss = 0.71363007\n",
      "Iteration 7, loss = 0.69353584\n",
      "Iteration 8, loss = 0.65803219\n",
      "Iteration 9, loss = 0.66833416\n",
      "Iteration 10, loss = 0.69501963\n",
      "Iteration 11, loss = 0.69402162\n",
      "Iteration 12, loss = 0.68026924\n",
      "Iteration 13, loss = 0.70782901\n",
      "Iteration 14, loss = 0.69999172\n",
      "Iteration 15, loss = 0.73920478\n",
      "Iteration 16, loss = 0.67819573\n",
      "Iteration 17, loss = 0.62472104\n",
      "Iteration 18, loss = 0.67507159\n",
      "Iteration 19, loss = 0.64945818\n",
      "Iteration 20, loss = 0.68958761\n",
      "Iteration 21, loss = 0.67441196\n",
      "Iteration 22, loss = 0.68279130\n",
      "Iteration 23, loss = 0.63351456\n",
      "Iteration 24, loss = 0.69140040\n",
      "Iteration 25, loss = 0.71266075\n",
      "Iteration 26, loss = 0.64952331\n",
      "Iteration 27, loss = 0.66957854\n",
      "Iteration 28, loss = 0.67700926\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.69594296\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 30, loss = 0.67847547\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.71231619\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.69995656\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 33, loss = 0.66206976\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 34, loss = 0.68769503\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.68058861\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 36, loss = 0.68491679\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.67017231\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.64894323\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.68460154\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.64064997\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.64520337\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 42, loss = 0.66733847\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 43, loss = 0.68754681\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 44, loss = 0.66612155\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.62852955\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.66130973\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 47, loss = 0.63196721\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 48, loss = 0.68183903\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 49, loss = 0.64942533\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 50, loss = 0.63613061\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 51, loss = 0.66204981\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 52, loss = 0.68260540\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.64830058\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.63590287\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.68526202\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.65612285\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.65111598\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.69289815\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.71449533\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.64374689\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 61, loss = 0.63280694\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 62, loss = 0.66770900\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.65511078\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 64, loss = 0.60768017\n",
      "Iteration 65, loss = 0.58132954\n",
      "Iteration 66, loss = 0.66905613\n",
      "Iteration 67, loss = 0.61855380\n",
      "Iteration 68, loss = 0.60752188\n",
      "Iteration 69, loss = 0.58616024\n",
      "Iteration 70, loss = 0.65122546\n",
      "Iteration 71, loss = 0.60965976\n",
      "Iteration 72, loss = 0.61298464\n",
      "Iteration 73, loss = 0.68174487\n",
      "Iteration 74, loss = 0.65817702\n",
      "Iteration 75, loss = 0.71585335\n",
      "Iteration 76, loss = 0.59003357\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 77, loss = 0.63424369\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 78, loss = 0.65935081\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 79, loss = 0.63727509\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 80, loss = 0.68210163\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 81, loss = 0.63926093\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 82, loss = 0.60952706\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 83, loss = 0.69894324\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 84, loss = 0.64738740\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 85, loss = 0.68932436\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 86, loss = 0.62582906\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 87, loss = 0.62092857\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 88, loss = 0.63002819\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 89, loss = 0.64548799\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 90, loss = 0.59559921\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 91, loss = 0.66774432\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 92, loss = 0.60750233\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 93, loss = 0.62071621\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 94, loss = 0.68065708\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 95, loss = 0.60608884\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 96, loss = 0.66498292\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 97, loss = 0.68807003\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 98, loss = 0.70078632\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 99, loss = 0.59038375\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 100, loss = 0.67053511\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 101, loss = 0.63032584\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.59441953\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 103, loss = 0.67445861\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 104, loss = 0.61650071\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 105, loss = 0.57540099\n",
      "Iteration 106, loss = 0.58242910\n",
      "Iteration 107, loss = 0.53310772\n",
      "Iteration 108, loss = 0.70481188\n",
      "Iteration 109, loss = 0.62946425\n",
      "Iteration 110, loss = 0.63829711\n",
      "Iteration 111, loss = 0.59506456\n",
      "Iteration 112, loss = 0.54184548\n",
      "Iteration 113, loss = 0.63285920\n",
      "Iteration 114, loss = 0.54413282\n",
      "Iteration 115, loss = 0.61125817\n",
      "Iteration 116, loss = 0.69115691\n",
      "Iteration 117, loss = 0.72348688\n",
      "Iteration 118, loss = 0.58169293\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 119, loss = 0.58586033\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.57598599\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 121, loss = 0.63367075\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 122, loss = 0.56119513\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 123, loss = 0.56784306\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 124, loss = 0.69678867\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.70575233\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 126, loss = 0.56713460\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 127, loss = 0.50998877\n",
      "Iteration 128, loss = 0.57609253\n",
      "Iteration 129, loss = 0.55734700\n",
      "Iteration 130, loss = 0.60635916\n",
      "Iteration 131, loss = 0.51511436\n",
      "Iteration 132, loss = 0.38919022\n",
      "Iteration 133, loss = 0.55918396\n",
      "Iteration 134, loss = 0.65028050\n",
      "Iteration 135, loss = 0.63954443\n",
      "Iteration 136, loss = 0.63068786\n",
      "Iteration 137, loss = 0.46475779\n",
      "Iteration 138, loss = 0.58588949\n",
      "Iteration 139, loss = 0.58147841\n",
      "Iteration 140, loss = 0.69501057\n",
      "Iteration 141, loss = 0.69256030\n",
      "Iteration 142, loss = 0.73787546\n",
      "Iteration 143, loss = 0.59539842\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 144, loss = 0.62529207\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 145, loss = 0.60547199\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 146, loss = 0.64757875\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 147, loss = 0.59166102\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 148, loss = 0.61044516\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 149, loss = 0.61094081\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 150, loss = 0.62854454\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 151, loss = 0.65980535\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 152, loss = 0.56672643\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 153, loss = 0.62879634\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 154, loss = 0.60379767\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 155, loss = 0.60351930\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 156, loss = 0.54481556\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 157, loss = 0.56424906\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 158, loss = 0.55077315\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 159, loss = 0.51374098\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 160, loss = 0.65285051\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 161, loss = 0.63625284\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 162, loss = 0.56323643\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 163, loss = 0.51283604\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 164, loss = 0.52039199\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 165, loss = 0.68837706\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 166, loss = 0.70945975\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 167, loss = 0.59782462\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 168, loss = 0.58417847\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 169, loss = 0.56675751\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 170, loss = 0.57546166\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 171, loss = 0.61551854\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 172, loss = 0.59223529\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 173, loss = 0.61559933\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 174, loss = 0.56502804\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 175, loss = 0.53591635\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 176, loss = 0.62758962\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 177, loss = 0.52643883\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 178, loss = 0.64222048\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 179, loss = 0.57729552\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 180, loss = 0.58169816\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 181, loss = 0.60686113\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 182, loss = 0.56877331\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 183, loss = 0.64583340\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 184, loss = 0.70168605\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 185, loss = 0.62389278\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 186, loss = 0.60996681\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 187, loss = 0.58823371\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 188, loss = 0.53274156\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 189, loss = 0.57429044\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 190, loss = 0.56971508\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 191, loss = 0.55894954\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 192, loss = 0.60293829\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 193, loss = 0.56870106\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 194, loss = 0.51285923\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 195, loss = 0.49804748\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 196, loss = 0.54848691\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 197, loss = 0.57717187\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 198, loss = 0.73084647\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 199, loss = 0.53538360\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 200, loss = 0.51740497\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 201, loss = 0.45890422\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 202, loss = 0.58653868\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 203, loss = 0.49601986\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 204, loss = 0.59681577\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 205, loss = 0.44457535\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 206, loss = 0.44396945\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 207, loss = 0.61298100\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 208, loss = 0.58263809\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 209, loss = 0.54407302\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 210, loss = 0.42384553\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 211, loss = 0.47291660\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 212, loss = 0.55170356\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 213, loss = 0.46605581\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 214, loss = 0.47174071\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 215, loss = 0.57114122\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 216, loss = 0.55785446\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 217, loss = 0.46873609\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 218, loss = 0.50211773\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 219, loss = 0.55201984\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 220, loss = 0.44865264\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 221, loss = 0.45504082\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 222, loss = 0.40862213\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 223, loss = 0.61439704\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 224, loss = 0.43374262\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 225, loss = 0.54401943\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 226, loss = 0.52242157\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 227, loss = 0.57508356\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 228, loss = 0.53118274\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 229, loss = 0.44619623\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 230, loss = 0.53098736\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 231, loss = 0.50623560\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 232, loss = 0.52683737\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 233, loss = 0.52099847\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 234, loss = 0.44736480\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 235, loss = 0.48432683\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 236, loss = 0.41385317\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 237, loss = 0.38906863\n",
      "Iteration 238, loss = 0.42145276\n",
      "Iteration 239, loss = 0.55777809\n",
      "Iteration 240, loss = 0.43652127\n",
      "Iteration 241, loss = 0.28241444\n",
      "Iteration 242, loss = 0.30212825\n",
      "Iteration 243, loss = 0.52801801\n",
      "Iteration 244, loss = 0.39514494\n",
      "Iteration 245, loss = 0.49447204\n",
      "Iteration 246, loss = 0.46425405\n",
      "Iteration 247, loss = 0.47808932\n",
      "Iteration 248, loss = 0.31936631\n",
      "Iteration 249, loss = 0.36358056\n",
      "Iteration 250, loss = 0.52230848\n",
      "Iteration 251, loss = 0.36008694\n",
      "Iteration 252, loss = 0.38356902\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 253, loss = 0.53462444\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 254, loss = 0.39317579\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 255, loss = 0.41661727\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 256, loss = 0.35025767\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 257, loss = 0.36220188\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 258, loss = 0.41820552\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 259, loss = 0.41837574\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 260, loss = 0.41556609\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 261, loss = 0.51196108\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 262, loss = 0.39326211\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 263, loss = 0.43773427\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 264, loss = 0.58743134\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 265, loss = 0.33999381\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 266, loss = 0.40168202\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 267, loss = 0.35958628\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 268, loss = 0.38811533\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 269, loss = 0.41656990\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 270, loss = 0.32361022\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 271, loss = 0.33047156\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 272, loss = 0.28351677\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 273, loss = 0.35033607\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 274, loss = 0.27392679\n",
      "Iteration 275, loss = 0.35882957\n",
      "Iteration 276, loss = 0.26708448\n",
      "Iteration 277, loss = 0.30518259\n",
      "Iteration 278, loss = 0.27504261\n",
      "Iteration 279, loss = 0.26028559\n",
      "Iteration 280, loss = 0.42419846\n",
      "Iteration 281, loss = 0.39133203\n",
      "Iteration 282, loss = 0.42039224\n",
      "Iteration 283, loss = 0.28472239\n",
      "Iteration 284, loss = 0.38478841\n",
      "Iteration 285, loss = 0.22742721\n",
      "Iteration 286, loss = 0.27105477\n",
      "Iteration 287, loss = 0.29077794\n",
      "Iteration 288, loss = 0.20471175\n",
      "Iteration 289, loss = 0.35456371\n",
      "Iteration 290, loss = 0.21160827\n",
      "Iteration 291, loss = 0.27159483\n",
      "Iteration 292, loss = 0.34500613\n",
      "Iteration 293, loss = 0.27484172\n",
      "Iteration 294, loss = 0.27468797\n",
      "Iteration 295, loss = 0.35975250\n",
      "Iteration 296, loss = 0.26107414\n",
      "Iteration 297, loss = 0.21574132\n",
      "Iteration 298, loss = 0.36817020\n",
      "Iteration 299, loss = 0.17100801\n",
      "Iteration 300, loss = 0.33860157\n",
      "Iteration 301, loss = 0.28186241\n",
      "Iteration 302, loss = 0.27538133\n",
      "Iteration 303, loss = 0.39994441\n",
      "Iteration 304, loss = 0.21193208\n",
      "Iteration 305, loss = 0.24012123\n",
      "Iteration 306, loss = 0.16618009\n",
      "Iteration 307, loss = 0.26481413\n",
      "Iteration 308, loss = 0.20490369\n",
      "Iteration 309, loss = 0.33525728\n",
      "Iteration 310, loss = 0.29720565\n",
      "Iteration 311, loss = 0.23772523\n",
      "Iteration 312, loss = 0.19477342\n",
      "Iteration 313, loss = 0.30195098\n",
      "Iteration 314, loss = 0.38050041\n",
      "Iteration 315, loss = 0.32394956\n",
      "Iteration 316, loss = 0.23223809\n",
      "Iteration 317, loss = 0.25351276\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 318, loss = 0.36035836\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 319, loss = 0.34427759\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 320, loss = 0.17832122\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 321, loss = 0.13589601\n",
      "Iteration 322, loss = 0.19007549\n",
      "Iteration 323, loss = 0.28463563\n",
      "Iteration 324, loss = 0.36818842\n",
      "Iteration 325, loss = 0.12828199\n",
      "Iteration 326, loss = 0.27379776\n",
      "Iteration 327, loss = 0.16104190\n",
      "Iteration 328, loss = 0.22161636\n",
      "Iteration 329, loss = 0.20233742\n",
      "Iteration 330, loss = 0.16727463\n",
      "Iteration 331, loss = 0.26847421\n",
      "Iteration 332, loss = 0.21618379\n",
      "Iteration 333, loss = 0.15219032\n",
      "Iteration 334, loss = 0.34272089\n",
      "Iteration 335, loss = 0.37800827\n",
      "Iteration 336, loss = 0.28285792\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 337, loss = 0.33566500\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 338, loss = 0.33823635\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 339, loss = 0.35745188\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 340, loss = 0.21604761\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 341, loss = 0.15555742\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 342, loss = 0.34114863\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 343, loss = 0.26600090\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 344, loss = 0.26498950\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 345, loss = 0.31601897\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 346, loss = 0.20388965\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 347, loss = 0.29547295\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 348, loss = 0.25658105\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 349, loss = 0.35364952\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 350, loss = 0.38849603\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 351, loss = 0.14727446\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 352, loss = 0.26137355\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 353, loss = 0.22495046\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 354, loss = 0.24142084\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 355, loss = 0.21886262\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 356, loss = 0.18472641\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 357, loss = 0.28235471\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 358, loss = 0.14895204\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 359, loss = 0.20630028\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 360, loss = 0.23415247\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 361, loss = 0.27995289\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 362, loss = 0.42262213\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 363, loss = 0.27807720\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 364, loss = 0.16018555\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 365, loss = 0.13970283\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 366, loss = 0.21878175\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 367, loss = 0.26206198\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 368, loss = 0.23230763\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 369, loss = 0.17050436\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 370, loss = 0.23857775\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 371, loss = 0.23721859\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 372, loss = 0.22213844\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 373, loss = 0.30041771\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 374, loss = 0.25776615\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 375, loss = 0.16934152\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 376, loss = 0.31756839\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 377, loss = 0.19841275\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 378, loss = 0.32749398\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 379, loss = 0.23800716\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 380, loss = 0.29506022\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 381, loss = 0.11830566\n",
      "Iteration 382, loss = 0.27817871\n",
      "Iteration 383, loss = 0.18623681\n",
      "Iteration 384, loss = 0.12002587\n",
      "Iteration 385, loss = 0.30168296\n",
      "Iteration 386, loss = 0.13728237\n",
      "Iteration 387, loss = 0.21604296\n",
      "Iteration 388, loss = 0.20614522\n",
      "Iteration 389, loss = 0.23842282\n",
      "Iteration 390, loss = 0.29128805\n",
      "Iteration 391, loss = 0.13010236\n",
      "Iteration 392, loss = 0.35838741\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 393, loss = 0.27176477\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 394, loss = 0.41892966\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 395, loss = 0.14494107\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 396, loss = 0.15162016\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 397, loss = 0.17379826\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 398, loss = 0.32161906\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 399, loss = 0.29174772\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 400, loss = 0.24011246\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 401, loss = 0.19482561\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 402, loss = 0.21910386\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 403, loss = 0.28951989\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 404, loss = 0.14445784\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 405, loss = 0.17720145\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 406, loss = 0.23966329\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 407, loss = 0.11375059\n",
      "Iteration 408, loss = 0.28586511\n",
      "Iteration 409, loss = 0.26831616\n",
      "Iteration 410, loss = 0.25418259\n",
      "Iteration 411, loss = 0.24122936\n",
      "Iteration 412, loss = 0.16131319\n",
      "Iteration 413, loss = 0.35168506\n",
      "Iteration 414, loss = 0.15410986\n",
      "Iteration 415, loss = 0.34537184\n",
      "Iteration 416, loss = 0.27880073\n",
      "Iteration 417, loss = 0.25784500\n",
      "Iteration 418, loss = 0.34166028\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 419, loss = 0.16667939\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 420, loss = 0.20783789\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 421, loss = 0.20049076\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 422, loss = 0.13797031\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 423, loss = 0.19065517\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 424, loss = 0.28409413\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 425, loss = 0.39907413\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 426, loss = 0.27712611\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 427, loss = 0.22213380\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 428, loss = 0.27271500\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 429, loss = 0.27770574\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 430, loss = 0.06479747\n",
      "Iteration 431, loss = 0.23181975\n",
      "Iteration 432, loss = 0.14307523\n",
      "Iteration 433, loss = 0.32581424\n",
      "Iteration 434, loss = 0.19751003\n",
      "Iteration 435, loss = 0.23131875\n",
      "Iteration 436, loss = 0.22962894\n",
      "Iteration 437, loss = 0.18894429\n",
      "Iteration 438, loss = 0.21272865\n",
      "Iteration 439, loss = 0.15959514\n",
      "Iteration 440, loss = 0.09481196\n",
      "Iteration 441, loss = 0.17374324\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 442, loss = 0.19024193\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 443, loss = 0.17447352\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 444, loss = 0.36540557\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 445, loss = 0.15880910\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 446, loss = 0.22053095\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 447, loss = 0.17858582\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 448, loss = 0.28085283\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 449, loss = 0.26811981\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 450, loss = 0.19500743\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 451, loss = 0.13247191\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 452, loss = 0.13335480\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 453, loss = 0.21308985\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 454, loss = 0.14392998\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 455, loss = 0.35182554\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 456, loss = 0.16528836\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 457, loss = 0.07759783\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 458, loss = 0.23606132\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 459, loss = 0.24026255\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 460, loss = 0.22028615\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 461, loss = 0.17564299\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 462, loss = 0.10947666\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 463, loss = 0.17136039\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 464, loss = 0.18531108\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 465, loss = 0.24452257\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 466, loss = 0.17130705\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 467, loss = 0.28381148\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 468, loss = 0.15600968\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 469, loss = 0.30468520\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 470, loss = 0.23197337\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 471, loss = 0.27163792\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 472, loss = 0.24839410\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 473, loss = 0.17976307\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 474, loss = 0.10702141\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 475, loss = 0.23753490\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 476, loss = 0.21487128\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 477, loss = 0.25312422\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 478, loss = 0.25655642\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 479, loss = 0.16584245\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 480, loss = 0.11734986\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Run specialist in all seeds\n",
    "for seed, transformed in manager.transformed.items():\n",
    "    specialist = Specialist(transformed, seed)\n",
    "    specialist.evolve_process(interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      stage  score\n",
      "0    1500.0     53\n",
      "1    1505.0     53\n",
      "2    1510.0     44\n",
      "3    1515.0     48\n",
      "4    1520.0     51\n",
      "..      ...    ...\n",
      "475  3875.0     93\n",
      "476  3880.0     93\n",
      "477  3885.0     93\n",
      "478  3890.0     93\n",
      "479  3895.0     93\n",
      "\n",
      "[480 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get Specialist Score data\n",
    "results = {}\n",
    "for seed in manager.transformed.keys():\n",
    "    results[seed] = pd.read_csv(f'../../data/specialist/evolution/{batch_size}_batch_{seed}_score.csv')\n",
    "\n",
    "# Example: get seed 10 isolated\n",
    "seed_10 = results.get('s10')\n",
    "print(seed_10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "456e324ffeb74f1f034f765c2a4bb10a44a088b16d5dd2a7c0d4e6be8a580f29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
