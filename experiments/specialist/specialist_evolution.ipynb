{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from specialist_data import transformed\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = transformed.data.copy()\n",
    "\n",
    "def set_classifier():\n",
    "    return  MLPClassifier(\n",
    "        hidden_layer_sizes=(64,64,64,64,64),\n",
    "        alpha=0.000001,\n",
    "        max_iter=2000,\n",
    "        activation=\"tanh\",\n",
    "        verbose=10,\n",
    "        random_state=42,\n",
    "        tol=0.000005\n",
    "    )\n",
    "\n",
    "def intervalToTrain(batch_stg, batch_size):\n",
    "    startToTrain = batch_stg * batch_size\n",
    "    return data.query(f'index > {startToTrain} and index < {startToTrain+batch_size}')\n",
    "\n",
    "def intervalToTest(batch_stg, batch_size):\n",
    "    startToTest = (batch_stg * batch_size) + batch_size\n",
    "    return data.query(f'index > {startToTest} and index < {startToTest+batch_size}')\n",
    "\n",
    "def evolve_stage(clf, batch_stg, batch_size):\n",
    "    train_data = intervalToTrain(batch_stg, batch_size)\n",
    "    test_data = intervalToTest(batch_stg, batch_size)\n",
    "    transformed.set_data(train_data)\n",
    "    clf = clf.partial_fit(transformed.X, transformed.level, ['bad', 'good'])\n",
    "    transformed.set_data(test_data)\n",
    "    return clf.score(transformed.X, transformed.level)\n",
    "\n",
    "def evolve_process(clf, seed, interval=1, min_limit=15, max_limit=40):\n",
    "    results = []\n",
    "    stages = []\n",
    "    to_int = 1/interval\n",
    "    max_gen = max_limit - (interval * to_int)\n",
    "    min_gen = min_limit * to_int\n",
    "    batch_size = 1000 * interval\n",
    "    for i in range(int(min_gen), int(max_gen*to_int), int(interval*to_int)):\n",
    "        r = evolve_stage(clf, i, batch_size)\n",
    "        results.append(int(r*100))\n",
    "        stages.append(i*batch_size)\n",
    "    df = pd.DataFrame({'stage': stages, 'score': results})\n",
    "    df.to_csv(f'./evolution/{int(batch_size)}/{seed}_score.csv')\n",
    "    return stages, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69410425\n",
      "Iteration 2, loss = 0.69487784\n",
      "Iteration 3, loss = 0.69577950\n",
      "Iteration 4, loss = 0.68192350\n",
      "Iteration 5, loss = 0.67635065\n",
      "Iteration 6, loss = 0.68969264\n",
      "Iteration 7, loss = 0.69523301\n",
      "Iteration 8, loss = 0.67455053\n",
      "Iteration 9, loss = 0.66246627\n",
      "Iteration 10, loss = 0.66987370\n",
      "Iteration 11, loss = 0.66983248\n",
      "Iteration 12, loss = 0.68534020\n",
      "Iteration 13, loss = 0.62707646\n",
      "Iteration 14, loss = 0.63042221\n",
      "Iteration 15, loss = 0.68210250\n",
      "Iteration 16, loss = 0.64248639\n",
      "Iteration 17, loss = 0.66428735\n",
      "Iteration 18, loss = 0.63257609\n",
      "Iteration 19, loss = 0.63842270\n",
      "Iteration 20, loss = 0.67343763\n",
      "Iteration 21, loss = 0.61698278\n",
      "Iteration 22, loss = 0.62077048\n",
      "Iteration 23, loss = 0.59294332\n",
      "Iteration 24, loss = 0.64018964\n",
      "Iteration 25, loss = 0.63967869\n",
      "Iteration 26, loss = 0.57503467\n",
      "Iteration 27, loss = 0.55925613\n",
      "Iteration 28, loss = 0.58808017\n",
      "Iteration 29, loss = 0.66541445\n",
      "Iteration 30, loss = 0.63981092\n",
      "Iteration 31, loss = 0.62229464\n",
      "Iteration 32, loss = 0.58728619\n",
      "Iteration 33, loss = 0.60988679\n",
      "Iteration 34, loss = 0.61962365\n",
      "Iteration 35, loss = 0.61557089\n",
      "Iteration 36, loss = 0.62979742\n",
      "Iteration 37, loss = 0.64293193\n",
      "Iteration 38, loss = 0.59540682\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.60575312\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 40, loss = 0.60976692\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 41, loss = 0.55313443\n",
      "Iteration 42, loss = 0.57484224\n",
      "Iteration 43, loss = 0.57183888\n",
      "Iteration 44, loss = 0.62588102\n",
      "Iteration 45, loss = 0.57677780\n",
      "Iteration 46, loss = 0.59177627\n",
      "Iteration 47, loss = 0.61547985\n",
      "Iteration 48, loss = 0.56073047\n",
      "Iteration 49, loss = 0.58004063\n",
      "Iteration 50, loss = 0.56871998\n",
      "Iteration 51, loss = 0.59096895\n",
      "Iteration 52, loss = 0.55893670\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.63098489\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 54, loss = 0.56441221\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 55, loss = 0.55456770\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 56, loss = 0.56358787\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 57, loss = 0.59477670\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 58, loss = 0.55671548\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 59, loss = 0.58213471\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 60, loss = 0.51751357\n",
      "Iteration 61, loss = 0.56001948\n",
      "Iteration 62, loss = 0.53788155\n",
      "Iteration 63, loss = 0.53950737\n",
      "Iteration 64, loss = 0.55244086\n",
      "Iteration 65, loss = 0.54036633\n",
      "Iteration 66, loss = 0.54193838\n",
      "Iteration 67, loss = 0.58475400\n",
      "Iteration 68, loss = 0.58448679\n",
      "Iteration 69, loss = 0.51846743\n",
      "Iteration 70, loss = 0.53393294\n",
      "Iteration 71, loss = 0.53441637\n",
      "Training loss did not improve more than tol=0.000005 for 10 consecutive epochs. Stopping.\n",
      "Iteration 72, loss = 0.48925211\n",
      "Iteration 73, loss = 0.58761570\n",
      "Iteration 74, loss = 0.50132748\n",
      "Iteration 75, loss = 0.50358626\n",
      "Iteration 76, loss = 0.51173037\n",
      "Iteration 77, loss = 0.48868799\n",
      "Iteration 78, loss = 0.51064758\n",
      "Iteration 79, loss = 0.47226822\n",
      "Iteration 80, loss = 0.48889338\n",
      "Iteration 81, loss = 0.50048034\n",
      "Iteration 82, loss = 0.52633036\n",
      "Iteration 83, loss = 0.48844923\n",
      "Iteration 84, loss = 0.44748215\n",
      "Iteration 85, loss = 0.51469298\n",
      "Iteration 86, loss = 0.45376983\n",
      "Iteration 87, loss = 0.46395322\n",
      "Iteration 88, loss = 0.39656693\n",
      "Iteration 89, loss = 0.38117310\n",
      "Iteration 90, loss = 0.41837353\n",
      "Iteration 91, loss = 0.41362081\n",
      "Iteration 92, loss = 0.34347321\n",
      "Iteration 93, loss = 0.37623167\n",
      "Iteration 94, loss = 0.36873410\n",
      "Iteration 95, loss = 0.35100333\n",
      "Iteration 96, loss = 0.33561328\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([15000.0,\n",
       "  15250.0,\n",
       "  15500.0,\n",
       "  15750.0,\n",
       "  16000.0,\n",
       "  16250.0,\n",
       "  16500.0,\n",
       "  16750.0,\n",
       "  17000.0,\n",
       "  17250.0,\n",
       "  17500.0,\n",
       "  17750.0,\n",
       "  18000.0,\n",
       "  18250.0,\n",
       "  18500.0,\n",
       "  18750.0,\n",
       "  19000.0,\n",
       "  19250.0,\n",
       "  19500.0,\n",
       "  19750.0,\n",
       "  20000.0,\n",
       "  20250.0,\n",
       "  20500.0,\n",
       "  20750.0,\n",
       "  21000.0,\n",
       "  21250.0,\n",
       "  21500.0,\n",
       "  21750.0,\n",
       "  22000.0,\n",
       "  22250.0,\n",
       "  22500.0,\n",
       "  22750.0,\n",
       "  23000.0,\n",
       "  23250.0,\n",
       "  23500.0,\n",
       "  23750.0,\n",
       "  24000.0,\n",
       "  24250.0,\n",
       "  24500.0,\n",
       "  24750.0,\n",
       "  25000.0,\n",
       "  25250.0,\n",
       "  25500.0,\n",
       "  25750.0,\n",
       "  26000.0,\n",
       "  26250.0,\n",
       "  26500.0,\n",
       "  26750.0,\n",
       "  27000.0,\n",
       "  27250.0,\n",
       "  27500.0,\n",
       "  27750.0,\n",
       "  28000.0,\n",
       "  28250.0,\n",
       "  28500.0,\n",
       "  28750.0,\n",
       "  29000.0,\n",
       "  29250.0,\n",
       "  29500.0,\n",
       "  29750.0,\n",
       "  30000.0,\n",
       "  30250.0,\n",
       "  30500.0,\n",
       "  30750.0,\n",
       "  31000.0,\n",
       "  31250.0,\n",
       "  31500.0,\n",
       "  31750.0,\n",
       "  32000.0,\n",
       "  32250.0,\n",
       "  32500.0,\n",
       "  32750.0,\n",
       "  33000.0,\n",
       "  33250.0,\n",
       "  33500.0,\n",
       "  33750.0,\n",
       "  34000.0,\n",
       "  34250.0,\n",
       "  34500.0,\n",
       "  34750.0,\n",
       "  35000.0,\n",
       "  35250.0,\n",
       "  35500.0,\n",
       "  35750.0,\n",
       "  36000.0,\n",
       "  36250.0,\n",
       "  36500.0,\n",
       "  36750.0,\n",
       "  37000.0,\n",
       "  37250.0,\n",
       "  37500.0,\n",
       "  37750.0,\n",
       "  38000.0,\n",
       "  38250.0,\n",
       "  38500.0,\n",
       "  38750.0],\n",
       " [44,\n",
       "  50,\n",
       "  59,\n",
       "  60,\n",
       "  60,\n",
       "  52,\n",
       "  53,\n",
       "  59,\n",
       "  63,\n",
       "  63,\n",
       "  57,\n",
       "  66,\n",
       "  65,\n",
       "  57,\n",
       "  63,\n",
       "  61,\n",
       "  64,\n",
       "  65,\n",
       "  63,\n",
       "  67,\n",
       "  67,\n",
       "  73,\n",
       "  65,\n",
       "  66,\n",
       "  73,\n",
       "  74,\n",
       "  71,\n",
       "  65,\n",
       "  64,\n",
       "  65,\n",
       "  73,\n",
       "  68,\n",
       "  65,\n",
       "  69,\n",
       "  65,\n",
       "  63,\n",
       "  71,\n",
       "  67,\n",
       "  67,\n",
       "  73,\n",
       "  71,\n",
       "  73,\n",
       "  67,\n",
       "  71,\n",
       "  71,\n",
       "  68,\n",
       "  70,\n",
       "  70,\n",
       "  70,\n",
       "  71,\n",
       "  71,\n",
       "  67,\n",
       "  73,\n",
       "  73,\n",
       "  73,\n",
       "  68,\n",
       "  75,\n",
       "  72,\n",
       "  77,\n",
       "  74,\n",
       "  75,\n",
       "  74,\n",
       "  73,\n",
       "  75,\n",
       "  71,\n",
       "  74,\n",
       "  72,\n",
       "  76,\n",
       "  76,\n",
       "  75,\n",
       "  78,\n",
       "  71,\n",
       "  77,\n",
       "  76,\n",
       "  75,\n",
       "  79,\n",
       "  78,\n",
       "  80,\n",
       "  78,\n",
       "  78,\n",
       "  75,\n",
       "  78,\n",
       "  80,\n",
       "  73,\n",
       "  81,\n",
       "  83,\n",
       "  90,\n",
       "  85,\n",
       "  81,\n",
       "  82,\n",
       "  87,\n",
       "  87,\n",
       "  88,\n",
       "  88,\n",
       "  88,\n",
       "  89])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = set_classifier()\n",
    "\n",
    "evolve_process(clf, seed=10, interval=0.25)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "456e324ffeb74f1f034f765c2a4bb10a44a088b16d5dd2a7c0d4e6be8a580f29"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
